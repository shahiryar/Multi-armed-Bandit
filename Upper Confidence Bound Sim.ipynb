{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed Bandit Problem\n",
    "\n",
    "The Multi-armed Bandit problem is a classic reinforcement learning problem that involves making a trade-off between exploration and exploitation. The name of the problem comes from the idea of a gambler playing a slot machine (a \"one-armed bandit\") with multiple levers (arms) that have different payoffs (reward distributions). The objective of the gambler is to maximize the total payoff over a number of plays.\n",
    "\n",
    "In the general case of the Multi-armed Bandit problem, there are N arms, and the reward distribution of each arm is unknown. The gambler must select which arm to pull at each time step, and observes the reward obtained. The goal is to maximize the total reward obtained over a fixed number of time steps, while simultaneously learning about the reward distributions of the arms.\n",
    "\n",
    "# Upper Confidence Bound (UCB) Algorithm\n",
    "\n",
    "One popular algorithm for solving the Multi-armed Bandit problem is the Upper Confidence Bound (UCB) algorithm. The basic idea behind UCB is to balance exploration and exploitation by selecting the arm with the highest upper confidence bound on the estimated mean reward. The UCB algorithm has been shown to have strong theoretical guarantees and performs well in practice.\n",
    "\n",
    "# Simulation with Normal Distributions\n",
    "\n",
    "In this notebook, we will simulate the Multi-armed Bandit problem with three options (A, B, and C) each with a normal distribution. We will use the UCB algorithm to select which option to choose at each time step, and compare its performance to a random selection strategy. The simulation will allow us to observe the trade-off between exploration and exploitation, and demonstrate the effectiveness of the UCB algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
